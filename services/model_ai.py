import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import requests
from typing import List, Tuple, Optional, Dict, Any
from pathlib import Path
import asyncio
import aiohttp
import gc  # Garbage collection ƒë·ªÉ gi·∫£i ph√≥ng RAM
from database import models_collection


class ModelAI:
    def __init__(self):
        """
        Kh·ªüi t·∫°o ModelAI class ƒë·ªÉ load model t·ª´ local file
        """
        self.current_model = None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.class_names = ["0", "1", "2", "3", "4", "5"]

        # Model directory v√† file path
        self.model_dir = Path("./ai_model")
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.model_path = self.model_dir / "best_checkpoint.pt"

        # Transform cho ·∫£nh (ImageNet pretrained)
        self.imagenet_mean = [0.485, 0.456, 0.406]
        self.imagenet_std = [0.229, 0.224, 0.225]
        self.size = (299, 299)

        self.transform = transforms.Compose(
            [
                transforms.Resize(self.size),
                transforms.ToTensor(),
                transforms.Normalize(mean=self.imagenet_mean, std=self.imagenet_std),
            ]
        )

        print(f"üß† ModelAI initialized")
        print(f"üìÇ Model path: {self.model_path}")
        print(f"üéØ Device: {self.device}")

        # Kh√¥ng t·ª± ƒë·ªông load model trong __init__, s·∫Ω load khi c·∫ßn
        print("üí° Model s·∫Ω ƒë∆∞·ª£c load khi c·∫ßn thi·∫øt")

    def _clear_model_from_memory(self):
        """
        X√≥a model kh·ªèi RAM v√† gi·∫£i ph√≥ng memory
        """
        if self.current_model is not None:
            # Move model to CPU tr∆∞·ªõc khi x√≥a (gi·∫£i ph√≥ng VRAM n·∫øu d√πng GPU)
            if hasattr(self.current_model, "cpu"):
                self.current_model.cpu()

            # X√≥a reference ƒë·∫øn model
            del self.current_model
            self.current_model = None

            # Clear CUDA cache n·∫øu d√πng GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # Force garbage collection ƒë·ªÉ gi·∫£i ph√≥ng RAM
            gc.collect()

            print("üóëÔ∏è ƒê√£ x√≥a model kh·ªèi RAM")

    def _create_resnet_with_dropout(
        self, num_classes=6, is_gray=False, dropout_prob=0.4
    ):
        """
        T·∫°o custom ResNet v·ªõi Dropout nh∆∞ c·ªßa anh
        """

        class ResNetWithDropout(nn.Module):
            def __init__(self, num_classes=6, is_gray=False, dropout_prob=0.4):
                super().__init__()
                self.is_gray = is_gray
                from torchvision import models

                self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
                if is_gray:
                    self.resnet.conv1 = nn.Conv2d(
                        1, 64, kernel_size=7, stride=2, padding=3, bias=False
                    )
                self.dropout = nn.Dropout(p=dropout_prob)
                self.fc = nn.Linear(512 * 4, num_classes)

            def forward(self, x):
                x = self.resnet.conv1(x)
                x = self.resnet.bn1(x)
                x = self.resnet.relu(x)
                x = self.resnet.maxpool(x)
                x = self.resnet.layer1(x)
                x = self.resnet.layer2(x)
                x = self.resnet.layer3(x)
                x = self.resnet.layer4(x)
                x = self.resnet.avgpool(x)
                x = torch.flatten(x, 1)
                x = self.dropout(x)
                return self.fc(x)

        print("üéØ Creating custom ResNetWithDropout model...")
        model = ResNetWithDropout(
            num_classes=num_classes, is_gray=is_gray, dropout_prob=dropout_prob
        )
        print("‚úÖ Custom ResNetWithDropout created successfully")
        return model

    def _create_inception_v3_with_dropout(
        self, num_classes=6, is_gray=False, dropout_prob=0.4
    ):
        """
        T·∫°o custom InceptionV3 v·ªõi Dropout
        """

        class InceptionV3WithDropout(nn.Module):
            def __init__(self, num_classes=6, is_gray=False, dropout_prob=0.2):
                super().__init__()
                self.is_gray = is_gray
                from torchvision import models

                # Load pretrained InceptionV3 with auxiliary logits enabled for training
                # Need specify weights=... and aux_logits=True for training
                self.model = models.inception_v3(
                    weights=models.Inception_V3_Weights.DEFAULT,
                    aux_logits=True,
                    transform_input=False,
                )

                if self.is_gray:
                    # InceptionV3 s·ª≠ d·ª•ng Conv2d_1a_3x3 thay v√¨ conv1
                    original_first_layer = self.model.Conv2d_1a_3x3.conv
                    self.model.Conv2d_1a_3x3.conv = nn.Conv2d(
                        1,  # T·ª´ 3 channels xu·ªëng 1 channel
                        original_first_layer.out_channels,
                        kernel_size=original_first_layer.kernel_size,
                        stride=original_first_layer.stride,
                        padding=original_first_layer.padding,
                        bias=False,
                    )

                # Replace main classifier with dropout + linear
                num_ftrs = self.model.fc.in_features
                self.model.fc = nn.Sequential(
                    nn.Dropout(p=dropout_prob), nn.Linear(num_ftrs, num_classes)
                )

                # Replace the auxiliary classifier (AuxLogits.fc)
                # Check if aux_logits is enabled
                if self.model.AuxLogits is not None:
                    num_aux_ftrs = self.model.AuxLogits.fc.in_features
                    self.model.AuxLogits.fc = nn.Sequential(
                        nn.Dropout(p=dropout_prob), nn.Linear(num_aux_ftrs, num_classes)
                    )
                else:
                    print(
                        "Warning: AuxLogits is None, auxiliary classifier not replaced."
                    )

            def forward(self, x):
                # InceptionV3 returns a tuple (output, aux_output) during training
                # and just output during evaluation. The train_model function handles this.
                return self.model(x)

        print("üéØ Creating custom InceptionV3WithDropout model...")
        model = InceptionV3WithDropout(
            num_classes=num_classes, is_gray=is_gray, dropout_prob=dropout_prob
        )
        print("‚úÖ Custom InceptionV3WithDropout created successfully")
        return model

    def _create_mobilenet_v2_with_dropout(
        self, num_classes=6, is_gray=False, dropout_prob=0.2
    ):
        """
        T·∫°o custom MobileNetV2 v·ªõi Dropout
        """

        class MobileNetV2WithDropout(nn.Module):
            def __init__(self, num_classes=6, is_gray=False, dropout_prob=0.2):
                super().__init__()
                self.is_gray = is_gray
                from torchvision import models

                # Load pretrained MobileNetV2
                self.model = models.mobilenet_v2(
                    weights=models.MobileNet_V2_Weights.DEFAULT
                )

                if self.is_gray:
                    # Thay ƒë·ªïi input layer t·ª´ 3 channels th√†nh 1 channel
                    original_first_layer = self.model.features[0][0]
                    self.model.features[0][0] = nn.Conv2d(
                        1,  # T·ª´ 3 channels xu·ªëng 1 channel
                        original_first_layer.out_channels,
                        kernel_size=original_first_layer.kernel_size,
                        stride=original_first_layer.stride,
                        padding=original_first_layer.padding,
                        bias=False,
                    )

                # Replace classifier v·ªõi dropout + linear
                num_ftrs = self.model.classifier[1].in_features
                self.model.classifier = nn.Sequential(
                    nn.Dropout(p=dropout_prob), nn.Linear(num_ftrs, num_classes)
                )

            def forward(self, x):
                return self.model(x)

        print("üéØ Creating custom MobileNetV2WithDropout model...")
        model = MobileNetV2WithDropout(
            num_classes=num_classes, is_gray=is_gray, dropout_prob=dropout_prob
        )
        print("‚úÖ Custom MobileNetV2WithDropout created successfully")
        return model

    async def load_model(self, current_model: dict) -> bool:
        """
        Load model t·ª´ file local backend/ai_model/best_checkpoint.pt

        Returns:
            bool: True n·∫øu load th√†nh c√¥ng
        """
        try:
            if not self.model_path.exists():
                print(f"‚ö†Ô∏è Model file kh√¥ng t·ªìn t·∫°i: {self.model_path}")
                print(f"Ti·∫øn h√†nh t·∫£i model t·ª´ {current_model['model_url']}")
                result_download = await self._download_model(
                    current_model["model_url"], self.model_path
                )

                if result_download:
                    print(f"‚úÖ T·∫£i model th√†nh c√¥ng t·ª´ {current_model['model_url']}")

            print(f"üîÑ Loading model t·ª´: {self.model_path}")

            # X√≥a model c≈© kh·ªèi RAM tr∆∞·ªõc
            self._clear_model_from_memory()

            model = None

            if current_model["name"] == "ResNet50":
                # T·∫°o custom ResNetWithDropout model
                model = self._create_resnet_with_dropout(
                    num_classes=6,  # BI-RADS 0-5
                    is_gray=False,  # RGB images (anh c√≥ th·ªÉ thay ƒë·ªïi th√†nh True n·∫øu d√πng grayscale)
                    dropout_prob=0.4,  # Dropout rate
                )
                # Load PyTorch checkpoint
                checkpoint = torch.load(self.model_path, map_location=self.device)
                state_dict = checkpoint["model_state_dict"]

                # Handle DataParallel prefix
                if list(state_dict.keys())[0].startswith("module."):
                    from collections import OrderedDict

                    state_dict = OrderedDict((k[7:], v) for k, v in state_dict.items())

                # Load state dict v√†o model
                model.load_state_dict(state_dict)

            elif current_model["name"] == "InceptionV3":
                model = self._create_inception_v3_with_dropout(
                    num_classes=6, is_gray=False, dropout_prob=0.2
                )
                checkpoint = torch.load(self.model_path, map_location=self.device)
                state_dict = checkpoint["model_state_dict"]
                if list(state_dict.keys())[0].startswith("module."):
                    from collections import OrderedDict

                    new_state_dict = OrderedDict()
                    for k, v in state_dict.items():
                        name = k[7:]
                        new_state_dict[name] = v
                    model.load_state_dict(new_state_dict)
                else:
                    model.load_state_dict(state_dict)

            elif current_model["name"] == "MobileNetV2":
                model = self._create_mobilenet_v2_with_dropout(
                    num_classes=6, is_gray=False, dropout_prob=0.2
                )
                checkpoint = torch.load(self.model_path, map_location=self.device)

                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p model ƒë∆∞·ª£c train b·∫±ng DataParallel
                state_dict = checkpoint["model_state_dict"]
                # Ki·ªÉm tra n·∫øu state_dict c√≥ ti·ªÅn t·ªë 'module.' (t·ª´ DataParallel)
                if list(state_dict.keys())[0].startswith("module."):
                    from collections import OrderedDict

                    new_state_dict = OrderedDict()
                    for k, v in state_dict.items():
                        name = k[7:]  # remove `module.`
                        new_state_dict[name] = v
                    model.load_state_dict(new_state_dict)
                else:
                    model.load_state_dict(state_dict)

            else:
                raise ValueError(f"Model {current_model['name']} kh√¥ng h·ª£p l·ªá")

            # Move model to device v√† set eval mode
            model = model.to(self.device)
            model.eval()

            # Clear checkpoint t·ª´ memory ƒë·ªÉ ti·∫øt ki·ªám RAM
            del checkpoint
            del state_dict
            gc.collect()

            self.current_model = model
            print(f"‚úÖ Model {current_model['name']} loaded th√†nh c√¥ng!")
            return True

        except Exception as e:
            print(f"‚ùå L·ªói load model: {e}")
            self._clear_model_from_memory()
            return False

    async def reload_model(self, model_url: str) -> bool:
        """
        Download model t·ª´ URL, thay th·∫ø model c≈© v√† load model m·ªõi

        Args:
            model_url: URL c·ªßa model c·∫ßn download

        Returns:
            bool: True n·∫øu reload th√†nh c√¥ng
        """
        try:
            print(f"üîÑ Reloading model t·ª´ URL: {model_url}")

            # X√≥a model c≈© kh·ªèi RAM tr∆∞·ªõc
            self._clear_model_from_memory()

            # X√≥a file model c≈© n·∫øu t·ªìn t·∫°i
            if self.model_path.exists():
                self.model_path.unlink()
                print(f"üóëÔ∏è ƒê√£ x√≥a model c≈©: {self.model_path}")

            # Download model m·ªõi t·ª´ URL
            print(f"üì• Downloading model t·ª´ URL...")
            success = await self._download_model(model_url, self.model_path)

            if not success:
                print("‚ùå Download model th·∫•t b·∫°i")
                return False

            # Load model m·ªõi
            print(f"üß† Loading model m·ªõi...")
            current_model = await self.get_model_info()
            if current_model:
                load_success = await self.load_model(current_model)
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y model active ƒë·ªÉ load")
                return False

            if load_success:
                print(f"‚úÖ Reload model th√†nh c√¥ng!")
                return True
            else:
                print("‚ùå Load model m·ªõi th·∫•t b·∫°i")
                return False

        except Exception as e:
            print(f"‚ùå L·ªói reload model: {e}")
            self._clear_model_from_memory()
            return False

    async def _download_model(self, model_url: str, save_path: Path) -> bool:
        """
        Download model t·ª´ URL v·ªõi retry v√† timeout
        """
        max_retries = 3
        timeout = 300  # 5 minutes timeout

        for attempt in range(max_retries):
            try:
                connector = aiohttp.TCPConnector(limit=10, ttl_dns_cache=300)
                timeout_config = aiohttp.ClientTimeout(total=timeout)

                async with aiohttp.ClientSession(
                    connector=connector, timeout=timeout_config
                ) as session:
                    async with session.get(model_url) as response:
                        if response.status == 200:
                            with open(save_path, "wb") as f:
                                async for chunk in response.content.iter_chunked(8192):
                                    f.write(chunk)

                            # Check file size
                            file_size = save_path.stat().st_size / (1024 * 1024)  # MB
                            print(
                                f"‚úÖ Downloaded: {save_path.name} ({file_size:.1f} MB)"
                            )
                            return True
                        else:
                            print(f"‚ùå HTTP {response.status}")

            except asyncio.TimeoutError:
                print(f"‚è∞ Timeout attempt {attempt + 1}/{max_retries}")
            except Exception as e:
                print(f"‚ùå Download error attempt {attempt + 1}/{max_retries}: {e}")

            if attempt < max_retries - 1:
                await asyncio.sleep(2**attempt)  # Exponential backoff

        return False

    async def predict_image(
        self, image_url: str
    ) -> Tuple[Optional[int], Optional[str], Optional[float], Optional[List[float]]]:
        """
        D·ª± ƒëo√°n ·∫£nh t·ª´ URL

        Args:
            image_url: URL c·ªßa ·∫£nh c·∫ßn d·ª± ƒëo√°n

        Returns:
            Tuple: (predicted_label_id, predicted_class_name, predicted_probability, all_probabilities)
        """
        # T·ª± ƒë·ªông initialize model n·∫øu ch∆∞a load
        if self.current_model is None:
            success = await self.initialize_model()
            if not success:
                print("‚ö†Ô∏è Kh√¥ng th·ªÉ load model")
                return None, None, None, None

        try:
            # Download ·∫£nh t·ª´ URL v·ªõi timeout
            timeout = 30  # 30 seconds timeout
            response = requests.get(image_url, stream=True, timeout=timeout)
            if response.status_code != 200:
                print(f"‚ùå HTTP {response.status_code} khi download ·∫£nh")
                return None, None, None, None

            # M·ªü ·∫£nh b·∫±ng PIL
            img = Image.open(response.raw)
            # img.save("test.jpg")
            image = img.convert("RGB")

            # Apply transform
            image = self.transform(image)
            image = image.unsqueeze(0)  # Add batch dimension
            image = image.to(self.device)

            # Predict
            self.current_model.eval()
            with torch.no_grad():
                outputs = self.current_model(image)

                # Handle InceptionV3 auxiliary output
                if isinstance(outputs, tuple):
                    outputs = outputs[0]

            # Get probabilities
            probabilities = torch.softmax(outputs, dim=1)
            predicted_probability, predicted_label_id = torch.max(probabilities, 1)

            # Get class name
            predicted_class_name = self.class_names[predicted_label_id.item()]

            # Convert all probabilities to list
            all_probabilities = probabilities[0].cpu().numpy().tolist()

            # Cleanup tensors ƒë·ªÉ ti·∫øt ki·ªám memory
            del image, outputs, probabilities
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return (
                predicted_label_id.item(),
                predicted_class_name,
                predicted_probability.item(),
                all_probabilities,
            )

        except requests.exceptions.Timeout:
            print("‚è∞ Timeout khi download ·∫£nh")
            return None, None, None, None
        except Exception as e:
            print(f"‚ùå L·ªói predict: {e}")
            return None, None, None, None

    async def predict(self, image_url: str) -> List[float]:
        """
        Predict v√† tr·∫£ v·ªÅ m·∫£ng x√°c su·∫•t cho 6 l·ªõp

        Args:
            image_url: URL c·ªßa ·∫£nh

        Returns:
            List[float]: M·∫£ng x√°c su·∫•t 6 ph·∫ßn t·ª≠ t∆∞∆°ng ·ª©ng v·ªõi l·ªõp 0-5
        """

        if not self.is_model_loaded():
            print("‚ùå Model ch∆∞a ƒë∆∞·ª£c load, kh√¥ng th·ªÉ d·ª± ƒëo√°n")
            raise Exception("Model ch∆∞a ƒë∆∞·ª£c load")

        _, _, _, all_probabilities = await self.predict_image(image_url)

        if all_probabilities is None:
            # Fallback: equal probability
            return [0.16, 0.17, 0.17, 0.17, 0.16, 0.17]

        return all_probabilities

    async def get_prediction_details(self, image_url: str) -> Dict:
        """
        L·∫•y th√¥ng tin d·ª± ƒëo√°n chi ti·∫øt

        Args:
            image_url: URL c·ªßa ·∫£nh

        Returns:
            Dict: Th√¥ng tin d·ª± ƒëo√°n chi ti·∫øt
        """
        predicted_id, predicted_class, predicted_prob, all_probs = (
            await self.predict_image(image_url)
        )

        if predicted_id is None:
            return {"success": False, "error": "Kh√¥ng th·ªÉ d·ª± ƒëo√°n ·∫£nh"}

        # Map class names to BI-RADS
        bi_rads_names = [
            "BI-RADS 0 - C·∫ßn ƒë√°nh gi√° th√™m",
            "BI-RADS 1 - √Çm t√≠nh",
            "BI-RADS 2 - T·ªïn th∆∞∆°ng l√†nh t√≠nh",
            "BI-RADS 3 - C√≥ th·ªÉ l√†nh t√≠nh",
            "BI-RADS 4 - Nghi ng·ªù √°c t√≠nh",
            "BI-RADS 5 - R·∫•t nghi ng·ªù √°c t√≠nh",
        ]

        # T·∫°o probability distribution
        probability_distribution = []
        for i, prob in enumerate(all_probs):
            probability_distribution.append(
                {
                    "class_id": i,
                    "class_name": bi_rads_names[i],
                    "probability": prob,
                    "percentage": prob * 100,
                }
            )

        # X√°c ƒë·ªãnh risk level
        if predicted_id <= 2:
            risk_level = "Low"
            recommendation = "Ti·∫øp t·ª•c t·∫ßm so√°t ƒë·ªãnh k·ª≥ theo l·ªãch"
        elif predicted_id == 3:
            risk_level = "Medium"
            recommendation = "Theo d√µi ng·∫Øn h·∫°n trong 6 th√°ng"
        else:
            risk_level = "High"
            recommendation = "C·∫ßn sinh thi·∫øt v√† thƒÉm kh√°m chuy√™n s√¢u ngay"

        return {
            "success": True,
            "predicted_class_id": predicted_id,
            "predicted_class_name": bi_rads_names[predicted_id],
            "confidence": predicted_prob,
            "probability_distribution": probability_distribution,
            "risk_level": risk_level,
            "recommendation": recommendation,
        }

    def is_model_loaded(self) -> bool:
        """
        Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c load ch∆∞a
        """
        return self.current_model is not None

    async def get_model_info(self) -> Dict:
        """
        L·∫•y th√¥ng tin model hi·ªán t·∫°i
        """
        # Query database ƒë·ªÉ l·∫•y th√¥ng tin model v·ªõi is_active = true
        model_info = await models_collection.find_one({"is_active": True})
        if not model_info:
            print("‚ùå Kh√¥ng t√¨m th·∫•y model active trong database")
            return None

        # print(f"üìã Model info t·ª´ database: {model_info}")
        return model_info

    async def initialize_model(self):
        """
        Initialize v√† load model khi c·∫ßn thi·∫øt
        """
        if self.current_model is not None:
            print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c load")
            return True

        current_model = await self.get_model_info()
        if current_model:
            print(f"üîÑ Loading model: {current_model['name']}")
            return await self.load_model(current_model)
        else:
            return False


# Singleton instance
model_ai = ModelAI()


# Function ƒë·ªÉ initialize model khi service start
async def initialize_model_service():
    """
    Initialize model khi service kh·ªüi ƒë·ªông
    """
    print("üöÄ Initializing model service...")

    result = await model_ai.initialize_model()
    if result:
        print("‚úÖ Model service initialized successfully")
        return True
    else:
        print("‚ùå Failed to initialize model service")
        return False


# H√†m test
async def test_model_ai():
    """
    Test ModelAI v·ªõi model local
    """
    print("üß™ Testing ModelAI...")

    # Initialize model service
    success = await initialize_model_service()
    if success:
        print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c load")
        model_info = await model_ai.get_model_info()
        print(f"üìã Model info: {model_info}")

        # Test v·ªõi sample image URL
        sample_url = "https://images.ctfassets.net/yixw23k2v6vo/duh3MNzrTx98KdhQj9iSL/e5bd997a82a5647766c62aa5e8924c1e/large-multifocal-breast-cancer-3000x2000.jpg"

        # Test predict
        probabilities = await model_ai.predict(sample_url)
        print(f"üìä Probabilities: {[f'{p:.3f}' for p in probabilities]}")

        # # Test detailed prediction
        # details = await model_ai.get_prediction_details(sample_url)
        # print(f"üìã Prediction details: {details}")

    else:
        print("‚ùå Model ch∆∞a ƒë∆∞·ª£c load")
        print("üí° H√£y ƒë·∫∑t model v√†o backend/ai_model/best_checkpoint.pt")


if __name__ == "__main__":
    asyncio.run(test_model_ai())
